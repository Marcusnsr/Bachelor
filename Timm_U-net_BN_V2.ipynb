{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a timm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install fastai\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"This code is not running in Google Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Ensure fallback is enabled\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm import create_model\n",
    "\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from fastai.callback.tracker import SaveModelCallback, EarlyStoppingCallback\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.metrics import Dice\n",
    "from fastai import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defineing the U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBnAct(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n",
    "                 stride=1, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = act_layer(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        conv_args = dict(kernel_size=3, padding=1, act_layer=act_layer)\n",
    "        self.scale_factor = scale_factor\n",
    "        if norm_layer is None:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels,  **conv_args)\n",
    "        else:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "\n",
    "    def forward(self, x, skip: Optional[torch.Tensor] = None):\n",
    "        if self.scale_factor != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            final_channels=1,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if center:\n",
    "            channels = encoder_channels[0]\n",
    "            self.center = DecoderBlock(channels, channels, scale_factor=1.0, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        in_channels = [in_chs + skip_chs for in_chs, skip_chs in zip(\n",
    "            [encoder_channels[0]] + list(decoder_channels[:-1]),\n",
    "            list(encoder_channels[1:]) + [0])]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for in_chs, out_chs in zip(in_channels, out_channels):\n",
    "            self.blocks.append(DecoderBlock(in_chs, out_chs, norm_layer=norm_layer))\n",
    "        self.final_conv = nn.Conv2d(out_channels[-1], final_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        encoder_head = x[0]\n",
    "        skips = x[1:]\n",
    "        x = self.center(encoder_head)\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = b(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used.\n",
    "        num_classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        center: if ``True`` add ``Conv2dReLU`` block on encoder head\n",
    "    NOTE: This is based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone='resnet50',\n",
    "            backbone_kwargs=None,\n",
    "            backbone_indices=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            in_chans=1,\n",
    "            num_classes=5,\n",
    "            center=False,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        backbone_kwargs = backbone_kwargs or {}\n",
    "        # NOTE some models need different backbone indices specified based on the alignment of features\n",
    "        # and some models won't have a full enough range of feature strides to work properly.\n",
    "        encoder = create_model(\n",
    "            backbone, features_only=True, out_indices=backbone_indices, in_chans=in_chans,\n",
    "            pretrained=True, **backbone_kwargs)\n",
    "        encoder_channels = encoder.feature_info.channels()[::-1]\n",
    "        self.encoder = encoder\n",
    "\n",
    "        if not decoder_use_batchnorm:\n",
    "            norm_layer = None\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            final_channels=num_classes,\n",
    "            norm_layer=norm_layer,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encoder(x)\n",
    "        x.reverse()  # torchscript doesn't work with [::-1]\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defineing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data/')\n",
    "\n",
    "def get_data(size, tfms=None, bs=64):\n",
    "    if not tfms:\n",
    "        tfms = [IntToFloatTensor(div_mask=255), \n",
    "                 FlipItem(p=0.5), \n",
    "                 Rotate(max_deg=180, p=0.5), \n",
    "                 Zoom(max_zoom=1.2, min_zoom=0.5, p=0.5, pad_mode='reflection')]\n",
    "    db = DataBlock(blocks=(ImageBlock(),MaskBlock()),\n",
    "                   splitter=RandomSplitter(valid_pct=0.1,seed=2021),\n",
    "                   batch_tfms=tfms,\n",
    "                   item_tfms=[Resize(size)],\n",
    "                   get_items=get_image_files, get_y=lambda o:str(o).replace('images','labels'))\n",
    "    return db.dataloaders(source=data_path/'train'/'images',bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss:\n",
    "    \"Dice and Focal combined\"\n",
    "    def __init__(self, axis=1, smooth=1., alpha=1., reduction='mean'):\n",
    "        store_attr()\n",
    "        self.focal_loss = FocalLossFlat(axis=axis)\n",
    "        self.dice_loss =  DiceLoss(axis, smooth, reduction)\n",
    "        \n",
    "    def __call__(self, pred, targ):\n",
    "        return self.focal_loss(pred, targ) + self.alpha * self.dice_loss(pred, targ)\n",
    "    \n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback = WandbCallback(\n",
    "    log='all', \n",
    "    log_preds=False,\n",
    "    log_preds_every_epoch=False, \n",
    "    log_model=False,\n",
    "    model_name=None, \n",
    "    log_dataset=False,\n",
    "    dataset_name=None,\n",
    "    valid_dl=None, \n",
    "    n_preds=36,\n",
    "    seed=12345, \n",
    "    reorder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_data(size=256,bs=64)\n",
    "dls.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Dice()\n",
    "loss_func = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner with combined loss and Dice metric\n",
    "learn = Learner(dls, Unet(backbone='resnet50', decoder_use_batchnorm=True, in_chans=3, num_classes=2),\n",
    "                loss_func=CombinedLoss(),\n",
    "                metrics=[metrics], cbs=[SaveModelCallback(monitor='dice', fname='best_model', comp=np.greater),\n",
    "                EarlyStoppingCallback(monitor='dice', min_delta=0.01, patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"Bachelor_WSI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"Bachelor_WSI\")\n",
    "\n",
    "# All images now with augmentations\n",
    "dls = get_data(size=256,bs=8)\n",
    "learn = Learner(dls,Unet(backbone='resnet50', decoder_use_batchnorm=True, in_chans=3, num_classes=2), loss_func=loss_func, metrics=dice_metric, cbs=[wandb_callback])\n",
    "learn.fine_tune(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "learn.show_results(max_n=1, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "learn.save('unet-resnet50-256v2_augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
